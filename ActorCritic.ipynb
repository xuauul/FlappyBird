{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line make pop-out window not appear\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "\n",
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
    "env.reset_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STACK = 4\n",
    "NUM_STATE = len(game.getGameState().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(self, num_action):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(units=128),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(units=256),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(units=512),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(units=256),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(units=128),\n",
    "            tf.keras.layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "        self.actor = tf.keras.layers.Dense(units=num_action)\n",
    "        self.critic = tf.keras.layers.Dense(units=1)\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.hidden_layers(state)\n",
    "        \n",
    "        actor_logits = self.actor(x)\n",
    "        actor_prob = tf.keras.layers.Softmax()(actor_logits)\n",
    "        \n",
    "        value = self.critic(x)\n",
    "        \n",
    "        return actor_prob, actor_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_action, discount_factor=0.99):\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_action = num_action\n",
    "        self.model = ActorCritic(num_action)\n",
    "        self.huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "    \n",
    "    def loss(self, state, action, returns, next_state):\n",
    "        \"\"\"\n",
    "        The actor-critic loss\n",
    "        \"\"\"\n",
    "        action_prob, action_logits, value = self.model(state)\n",
    "        index = tf.stack([tf.range(tf.shape(action)[0]), action], axis=1)\n",
    "        action_prob = tf.gather_nd(action_prob, index)\n",
    "        \n",
    "        advantage = returns - value\n",
    "        \n",
    "        action_log_prob = tf.math.log(action_prob)\n",
    "        actor_loss = -tf.math.reduce_sum(action_log_prob * advantage)\n",
    "        \n",
    "        critic_loss = self.huber_loss(value, returns)\n",
    "        \n",
    "        return actor_loss + critic_loss\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        # convert state into a batched tensor\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        \n",
    "        action_prob, action_logits, value = self.model(state)\n",
    "        \n",
    "        # sample next action from the action probability distribution\n",
    "        action = tf.random.categorical(action_logits, 1)[0, 0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init agent\n",
    "num_action = len(env.getActionSet())\n",
    "\n",
    "# agent for frequently updating\n",
    "AC_Agent = Agent(num_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-7)\n",
    "\n",
    "# @tf.function\n",
    "def train_step(state, action, returns, next_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = AC_Agent.loss(state, action, returns, next_state)\n",
    "    gradients = tape.gradient(loss, AC_Agent.model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, AC_Agent.model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    def __init__(self, gamma, buffer_size=50000):\n",
    "        self.experiences = []\n",
    "        self.gamma = gamma\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.experiences) >= self.buffer_size:\n",
    "            self.experiences.pop(0)\n",
    "        self.experiences.append(experience)\n",
    "        \n",
    "    def get_expected_returns(self):\n",
    "        \"\"\"\n",
    "        Computing expected returns\n",
    "        \"\"\"\n",
    "        rewards = [e[2] for e in self.experiences][::-1]\n",
    "        \n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        discounted_sum = 0.0\n",
    "        for i in range(len(rewards)):\n",
    "            discounted_sum = rewards[i] + self.gamma * discounted_sum\n",
    "            discounted_rewards[i] = discounted_sum\n",
    "        discounted_rewards = discounted_rewards[::-1]\n",
    "\n",
    "        # standardize\n",
    "        discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-7)\n",
    "        \n",
    "        return discounted_rewards\n",
    "        \n",
    "    def get_experiences(self):\n",
    "        returns = self.get_expected_returns()\n",
    "        \n",
    "        states, actions, states_prime = [], [], []\n",
    "        for state, action, reward, state_prime in self.experiences:\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            states_prime.append(state_prime)\n",
    "\n",
    "        return states, actions, returns, states_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "\n",
    "def make_anim(images, fps=60, true_image=False):\n",
    "    duration = len(images) / fps\n",
    "\n",
    "    def make_frame(t):\n",
    "        try:\n",
    "            x = images[int(len(images) / duration * t)]\n",
    "        except:\n",
    "            x = images[-1]\n",
    "\n",
    "        if true_image:\n",
    "            return x.astype(np.uint8)\n",
    "        else:\n",
    "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "    clip.fps = fps\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.transform\n",
    "\n",
    "def preprocess_screen(screen):\n",
    "    screen = skimage.transform.resize(screen, [IMG_WIDTH, IMG_HEIGHT, 1])\n",
    "    return screen\n",
    "\n",
    "def frames_to_state(input_frames):\n",
    "    if(len(input_frames) == 1):\n",
    "        state = np.concatenate(input_frames*4, axis=-1)\n",
    "    elif(len(input_frames) == 2):\n",
    "        state = np.concatenate(input_frames[0:1]*2 + input_frames[1:]*2, axis=-1)\n",
    "    elif(len(input_frames) == 3):\n",
    "        state = np.concatenate(input_frames + input_frames[2:], axis=-1)\n",
    "    else:\n",
    "        state = np.concatenate(input_frames[-4:], axis=-1)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "update_every_episode = 1\n",
    "print_every_episode = 10\n",
    "save_video_every_episode = 100\n",
    "NUM_EPISODE = 10000\n",
    "\n",
    "for episode in range(0, NUM_EPISODE + 1):\n",
    "    \n",
    "    # reset the environment\n",
    "    env.reset_game()\n",
    "    \n",
    "    # record frame\n",
    "    if episode % save_video_every_episode == 0:\n",
    "        frames = [env.getScreenRGB()]\n",
    "        \n",
    "    # input frame\n",
    "    # --------------------------------------------------\n",
    "    # (1, 84, 84, 1)\n",
    "    # input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
    "    \n",
    "    # (4, 8)\n",
    "    input_states = [list(game.getGameState().values())] * 4\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    # cumulate reward for this episode\n",
    "    cum_reward = 0\n",
    "    \n",
    "    # init buffer\n",
    "    buffer = Replay_buffer(AC_Agent.discount_factor)\n",
    "    \n",
    "    t = 0\n",
    "    while not env.game_over():\n",
    "        \n",
    "        # --------------------------------------------------\n",
    "        # state = frames_to_state(input_frames)\n",
    "        state = input_states[-4:]\n",
    "        # --------------------------------------------------\n",
    "        \n",
    "        # select action using current policy\n",
    "        action = AC_Agent.select_action(state)\n",
    "        \n",
    "        # execute the action and get reward\n",
    "        reward = env.act(env.getActionSet()[action])\n",
    "        \n",
    "        # record frame\n",
    "        if episode % save_video_every_episode == 0:\n",
    "            frames.append(env.getScreenRGB())\n",
    "            \n",
    "        # record input frame\n",
    "        # --------------------------------------------------\n",
    "        # input_frames.append(preprocess_screen(env.getScreenGrayscale()))\n",
    "        input_states.append(list(game.getGameState().values()))\n",
    "        # --------------------------------------------------\n",
    "        \n",
    "        # cumulate reward\n",
    "        cum_reward += reward\n",
    "        \n",
    "        # observe the result\n",
    "        # --------------------------------------------------\n",
    "        # state_prime = frames_to_state(input_frames) # get next state\n",
    "        state_prime = input_states[-4:] # get next state\n",
    "        # --------------------------------------------------\n",
    "        \n",
    "        # append experience for this episode\n",
    "        buffer.add((state, action, reward, state_prime))\n",
    "        \n",
    "        t += 1\n",
    "        \n",
    "    # update agent\n",
    "    train_states, train_actions, train_returns, train_states_prime = buffer.get_experiences()\n",
    "    # --------------------------------------------------\n",
    "    # train_states = np.asarray(train_states).reshape(-1, IMG_WIDTH, IMG_HEIGHT, NUM_STACK)\n",
    "    # train_states_prime = np.asarray(train_states_prime).reshape(-1, IMG_WIDTH, IMG_HEIGHT, NUM_STACK)\n",
    "\n",
    "    train_states = np.asarray(train_states).reshape(-1, NUM_STACK, NUM_STATE)\n",
    "    train_states_prime = np.asarray(train_states_prime).reshape(-1, NUM_STACK, NUM_STATE)\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    # convert Python object to Tensor to prevent graph re-tracing\n",
    "    train_states = tf.convert_to_tensor(train_states, tf.float32)\n",
    "    train_actions = tf.convert_to_tensor(train_actions, tf.int32)\n",
    "    train_returns = tf.convert_to_tensor(train_returns, tf.float32)\n",
    "    train_states_prime = tf.convert_to_tensor(train_states_prime, tf.float32)\n",
    "    \n",
    "    loss = train_step(train_states, train_actions, train_returns, train_states_prime)\n",
    "    \n",
    "    if episode % print_every_episode == 0:\n",
    "        print(\n",
    "            \"[{}] time live:{}, cumulated reward: {}, loss: {}\".\n",
    "            format(episode, t, cum_reward, loss))\n",
    "\n",
    "    if episode % save_video_every_episode == 0:  # for every 500 episode, record an animation\n",
    "        clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
    "        clip.write_videofile(\"movie_f/PG-{}.mp4\".format(episode), fps=60)\n",
    "        display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
